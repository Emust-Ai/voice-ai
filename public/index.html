<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Voice Agent - Web Client</title>
  <style>
    * {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }

    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
      background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
      min-height: 100vh;
      color: #fff;
      display: flex;
      flex-direction: column;
      align-items: center;
      padding: 20px;
    }

    .container {
      max-width: 600px;
      width: 100%;
    }

    h1 {
      text-align: center;
      margin-bottom: 10px;
      font-size: 1.8rem;
      background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
    }

    .subtitle {
      text-align: center;
      color: #888;
      margin-bottom: 30px;
      font-size: 0.9rem;
    }

    .status-bar {
      background: rgba(255,255,255,0.1);
      border-radius: 10px;
      padding: 15px;
      margin-bottom: 20px;
      display: flex;
      align-items: center;
      gap: 10px;
    }

    .status-indicator {
      width: 12px;
      height: 12px;
      border-radius: 50%;
      background: #666;
      transition: background 0.3s;
    }

    .status-indicator.connecting { background: #f59e0b; }
    .status-indicator.ready { background: #10b981; }
    .status-indicator.speaking { background: #3b82f6; animation: pulse 1s infinite; }
    .status-indicator.listening { background: #ef4444; animation: pulse 0.5s infinite; }
    .status-indicator.error { background: #ef4444; }

    @keyframes pulse {
      0%, 100% { opacity: 1; transform: scale(1); }
      50% { opacity: 0.7; transform: scale(1.1); }
    }

    .status-text {
      flex: 1;
      font-size: 0.9rem;
    }

    .controls {
      display: flex;
      justify-content: center;
      gap: 15px;
      margin-bottom: 20px;
    }

    .btn {
      padding: 15px 30px;
      border: none;
      border-radius: 50px;
      font-size: 1rem;
      font-weight: 600;
      cursor: pointer;
      transition: all 0.3s;
      display: flex;
      align-items: center;
      gap: 8px;
    }

    .btn-primary {
      background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
      color: white;
    }

    .btn-primary:hover:not(:disabled) {
      transform: translateY(-2px);
      box-shadow: 0 5px 20px rgba(102, 126, 234, 0.4);
    }

    .btn-danger {
      background: linear-gradient(90deg, #ef4444 0%, #dc2626 100%);
      color: white;
    }

    .btn-danger:hover:not(:disabled) {
      transform: translateY(-2px);
      box-shadow: 0 5px 20px rgba(239, 68, 68, 0.4);
    }

    .btn:disabled {
      opacity: 0.5;
      cursor: not-allowed;
    }

    .mic-btn {
      width: 80px;
      height: 80px;
      border-radius: 50%;
      font-size: 2rem;
      padding: 0;
      display: flex;
      align-items: center;
      justify-content: center;
    }

    .mic-btn.active {
      background: linear-gradient(90deg, #ef4444 0%, #dc2626 100%);
      animation: pulse 1s infinite;
    }

    .conversation {
      background: rgba(255,255,255,0.05);
      border-radius: 15px;
      padding: 20px;
      height: 400px;
      overflow-y: auto;
      margin-bottom: 20px;
    }

    .message {
      margin-bottom: 15px;
      padding: 12px 16px;
      border-radius: 12px;
      max-width: 85%;
      animation: fadeIn 0.3s;
    }

    @keyframes fadeIn {
      from { opacity: 0; transform: translateY(10px); }
      to { opacity: 1; transform: translateY(0); }
    }

    .message.user {
      background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
      margin-left: auto;
      text-align: right;
    }

    .message.assistant {
      background: rgba(255,255,255,0.1);
      margin-right: auto;
    }

    .message.system {
      background: rgba(245, 158, 11, 0.2);
      border: 1px solid rgba(245, 158, 11, 0.3);
      text-align: center;
      max-width: 100%;
      font-size: 0.85rem;
      color: #f59e0b;
    }

    .message .role {
      font-size: 0.75rem;
      opacity: 0.7;
      margin-bottom: 4px;
    }

    .visualizer {
      display: flex;
      justify-content: center;
      align-items: center;
      gap: 3px;
      height: 40px;
      margin-bottom: 20px;
    }

    .visualizer-bar {
      width: 4px;
      height: 10px;
      background: #667eea;
      border-radius: 2px;
      transition: height 0.1s;
    }

    .visualizer.active .visualizer-bar {
      animation: visualize 0.5s ease-in-out infinite;
    }

    .visualizer-bar:nth-child(1) { animation-delay: 0s; }
    .visualizer-bar:nth-child(2) { animation-delay: 0.1s; }
    .visualizer-bar:nth-child(3) { animation-delay: 0.2s; }
    .visualizer-bar:nth-child(4) { animation-delay: 0.3s; }
    .visualizer-bar:nth-child(5) { animation-delay: 0.4s; }
    .visualizer-bar:nth-child(6) { animation-delay: 0.3s; }
    .visualizer-bar:nth-child(7) { animation-delay: 0.2s; }
    .visualizer-bar:nth-child(8) { animation-delay: 0.1s; }

    @keyframes visualize {
      0%, 100% { height: 10px; }
      50% { height: 35px; }
    }

    .footer {
      text-align: center;
      color: #666;
      font-size: 0.8rem;
      margin-top: auto;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>üéôÔ∏è Voice Agent</h1>
    <p class="subtitle">Web Browser Testing Client</p>

    <div class="status-bar">
      <div class="status-indicator" id="statusIndicator"></div>
      <span class="status-text" id="statusText">Click "Start Call" to begin</span>
    </div>

    <div class="visualizer" id="visualizer">
      <div class="visualizer-bar"></div>
      <div class="visualizer-bar"></div>
      <div class="visualizer-bar"></div>
      <div class="visualizer-bar"></div>
      <div class="visualizer-bar"></div>
      <div class="visualizer-bar"></div>
      <div class="visualizer-bar"></div>
      <div class="visualizer-bar"></div>
    </div>

    <div class="controls">
      <button class="btn btn-primary" id="startBtn" onclick="startCall()">
        üìû Start Call
      </button>
      <button class="btn mic-btn btn-primary" id="micBtn" onclick="toggleMic()" disabled>
        üé§
      </button>
      <button class="btn btn-danger" id="endBtn" onclick="endCall()" disabled>
        üìµ End Call
      </button>
    </div>

    <div class="conversation" id="conversation">
      <div class="message system">
        Click "Start Call" to connect to the Voice Agent
      </div>
    </div>
  </div>

  <div class="footer">
    <p>Voice Agent Web Client | Audio is processed in real-time</p>
  </div>

  <script>
    let ws = null;
    let audioContext = null;
    let mediaStream = null;
    let mediaRecorder = null;
    let audioWorklet = null;
    let isRecording = false;
    let audioQueue = [];
    let isPlaying = false;
    let nextPlayTime = 0;
    let isAssistantSpeaking = false;
    let currentAudioSource = null; // Track current playing audio source

    const statusIndicator = document.getElementById('statusIndicator');
    const statusText = document.getElementById('statusText');
    const conversation = document.getElementById('conversation');
    const startBtn = document.getElementById('startBtn');
    const endBtn = document.getElementById('endBtn');
    const micBtn = document.getElementById('micBtn');
    const visualizer = document.getElementById('visualizer');

    function setStatus(status, text) {
      statusIndicator.className = 'status-indicator ' + status;
      statusText.textContent = text;
    }

    function addMessage(role, text) {
      const div = document.createElement('div');
      div.className = 'message ' + role;
      div.innerHTML = `<div class="role">${role.toUpperCase()}</div>${text}`;
      conversation.appendChild(div);
      conversation.scrollTop = conversation.scrollHeight;
    }

    async function startCall() {
      try {
        setStatus('connecting', 'Connecting...');
        startBtn.disabled = true;

        // Request microphone access
        mediaStream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            sampleRate: 24000,
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          } 
        });

        // Create audio context for playback
        audioContext = new AudioContext({ sampleRate: 24000 });

        // Connect WebSocket
        const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
        const wsUrl = `${protocol}//${window.location.host}/web-stream`;
        
        ws = new WebSocket(wsUrl);

        ws.onopen = () => {
          console.log('WebSocket connected');
          setStatus('connecting', 'Connected, initializing...');
          endBtn.disabled = false;
          addMessage('system', 'Connected to Voice Agent');
        };

        ws.onmessage = (event) => {
          const message = JSON.parse(event.data);
          handleServerMessage(message);
        };

        ws.onclose = () => {
          console.log('WebSocket closed');
          cleanup();
          setStatus('', 'Disconnected');
          addMessage('system', 'Call ended');
        };

        ws.onerror = (error) => {
          console.error('WebSocket error:', error);
          setStatus('error', 'Connection error');
          cleanup();
        };

      } catch (error) {
        console.error('Error starting call:', error);
        setStatus('error', 'Failed to start: ' + error.message);
        startBtn.disabled = false;
      }
    }

    function handleServerMessage(message) {
      switch (message.type) {
        case 'status':
          if (message.status === 'ready') {
            setStatus('ready', 'Ready - Click mic to speak');
            micBtn.disabled = false;
          } else if (message.status === 'disconnected') {
            cleanup();
          }
          break;

        case 'audio':
          queueAudio(message.audio);
          break;

        case 'audio_done':
          // Don't immediately set ready - wait for audio queue to finish
          break;

        case 'speech_started':
          setStatus('listening', 'Listening...');
          // Stop any playing audio when user starts speaking (barge-in)
          stopPlayback();
          break;

        case 'speech_stopped':
          setStatus('ready', 'Processing...');
          break;

        case 'transcript':
          addMessage(message.role, message.text);
          break;

        case 'text_delta':
          // Could show real-time text here
          break;

        case 'response_done':
          // Audio queue will handle setting status back to ready
          break;

        case 'tool_call':
          if (message.status === 'executing') {
            addMessage('system', `üîß Executing: ${message.name}`);
          }
          break;

        case 'error':
          setStatus('error', 'Error: ' + message.message);
          addMessage('system', '‚ö†Ô∏è Error: ' + message.message);
          break;
      }
    }

    async function toggleMic() {
      if (isRecording) {
        stopRecording();
      } else {
        startRecording();
      }
    }

    async function startRecording() {
      if (!mediaStream || !ws) return;
      
      // If assistant is speaking, stop playback first (barge-in)
      if (isAssistantSpeaking) {
        stopPlayback();
      }

      isRecording = true;
      micBtn.classList.add('active');
      setStatus('listening', 'Listening...');
      visualizer.classList.add('active');

      // Create audio processing
      const audioInput = audioContext.createMediaStreamSource(mediaStream);
      
      // Use ScriptProcessor for audio capture (simpler approach)
      const bufferSize = 4096;
      const scriptProcessor = audioContext.createScriptProcessor(bufferSize, 1, 1);
      
      scriptProcessor.onaudioprocess = (e) => {
        if (!isRecording || ws.readyState !== WebSocket.OPEN) return;
        
        const inputData = e.inputBuffer.getChannelData(0);
        
        // Resample from audioContext.sampleRate to 24000 if needed
        const targetSampleRate = 24000;
        let processedData;
        
        if (audioContext.sampleRate !== targetSampleRate) {
          const ratio = audioContext.sampleRate / targetSampleRate;
          const newLength = Math.round(inputData.length / ratio);
          processedData = new Float32Array(newLength);
          for (let i = 0; i < newLength; i++) {
            processedData[i] = inputData[Math.round(i * ratio)];
          }
        } else {
          processedData = inputData;
        }
        
        // Convert to PCM16
        const pcm16 = new Int16Array(processedData.length);
        for (let i = 0; i < processedData.length; i++) {
          const s = Math.max(-1, Math.min(1, processedData[i]));
          pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
        }
        
        // Convert to base64
        const base64 = btoa(String.fromCharCode(...new Uint8Array(pcm16.buffer)));
        
        // Send to server
        ws.send(JSON.stringify({ type: 'audio', audio: base64 }));
      };

      audioInput.connect(scriptProcessor);
      scriptProcessor.connect(audioContext.destination);
      
      // Store for cleanup
      audioWorklet = { scriptProcessor, audioInput };
    }

    function stopRecording() {
      isRecording = false;
      micBtn.classList.remove('active');
      if (!isAssistantSpeaking) {
        setStatus('ready', 'Processing...');
      }

      if (audioWorklet) {
        audioWorklet.scriptProcessor.disconnect();
        audioWorklet.audioInput.disconnect();
        audioWorklet = null;
      }
    }

    // Queue audio for sequential playback (prevents overlapping)
    function queueAudio(base64Audio) {
      audioQueue.push(base64Audio);
      if (!isPlaying) {
        playNextInQueue();
      }
    }

    // Stop all audio playback
    function stopPlayback() {
      audioQueue = [];
      isPlaying = false;
      isAssistantSpeaking = false;
      nextPlayTime = 0;
      
      // Stop currently playing audio source
      if (currentAudioSource) {
        try {
          currentAudioSource.stop();
        } catch (e) {
          // Already stopped
        }
        currentAudioSource = null;
      }
      
      visualizer.classList.remove('active');
    }

    // Play audio chunks sequentially
    async function playNextInQueue() {
      if (audioQueue.length === 0) {
        isPlaying = false;
        isAssistantSpeaking = false;
        visualizer.classList.remove('active');
        setStatus('ready', 'Ready - Click mic to speak');
        return;
      }

      isPlaying = true;
      isAssistantSpeaking = true;
      setStatus('speaking', 'Assistant speaking...');
      visualizer.classList.add('active');

      const base64Audio = audioQueue.shift();

      try {
        // Decode base64 to PCM16
        const binaryString = atob(base64Audio);
        const bytes = new Uint8Array(binaryString.length);
        for (let i = 0; i < binaryString.length; i++) {
          bytes[i] = binaryString.charCodeAt(i);
        }
        
        // Convert PCM16 to Float32
        const pcm16 = new Int16Array(bytes.buffer);
        const float32 = new Float32Array(pcm16.length);
        for (let i = 0; i < pcm16.length; i++) {
          float32[i] = pcm16[i] / 32768;
        }

        // Create audio buffer and play
        const audioBuffer = audioContext.createBuffer(1, float32.length, 24000);
        audioBuffer.getChannelData(0).set(float32);

        const source = audioContext.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(audioContext.destination);
        
        // Track current source so we can stop it on interruption
        currentAudioSource = source;
        
        // When this chunk ends, play next
        source.onended = () => {
          if (currentAudioSource === source) {
            currentAudioSource = null;
          }
          // Only continue if not interrupted
          if (isPlaying) {
            playNextInQueue();
          }
        };
        
        source.start();
        
      } catch (error) {
        console.error('Error playing audio:', error);
        playNextInQueue(); // Try next chunk
      }
    }

    async function playAudio(base64Audio) {
      queueAudio(base64Audio);
    }

    function endCall() {
      if (ws) {
        ws.send(JSON.stringify({ type: 'end_session' }));
        ws.close();
      }
      cleanup();
    }

    function cleanup() {
      stopRecording();
      stopPlayback();
      
      if (mediaStream) {
        mediaStream.getTracks().forEach(track => track.stop());
        mediaStream = null;
      }
      
      if (audioContext) {
        audioContext.close();
        audioContext = null;
      }
      
      ws = null;
      nextPlayTime = 0;
      currentAudioSource = null;
      startBtn.disabled = false;
      endBtn.disabled = true;
      micBtn.disabled = true;
      micBtn.classList.remove('active');
      visualizer.classList.remove('active');
    }

    // Cleanup on page unload
    window.addEventListener('beforeunload', cleanup);
  </script>
</body>
</html>
